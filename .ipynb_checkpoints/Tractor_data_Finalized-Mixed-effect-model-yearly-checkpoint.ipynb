{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , power_transform#\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, RANSACRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "from scipy.stats import skew\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as skn\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "import libpysal as lp\n",
    "import geopandas\n",
    "import shapely.geometry as shp\n",
    "\n",
    "from libpysal.weights.util import fill_diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_model = 1\n",
    "# No_fixed_effect_model = 0, fixed_effect_model = 1, Mixed_effect_model = 2\n",
    "\n",
    "linear_model_type = 0\n",
    "# LinearRegression = 0, RANSACRegressor = 1, LinearRegression_Ridge = 2, LinearRegression_Ridge = 3, LinearRegression_Elastic = 4\n",
    "\n",
    "spatial_regression = 0 \n",
    "# without Spatial Regression = 0, Spatial Regression = 1\n",
    "\n",
    "spatial_numfeatures = 0\n",
    "# 0: considering all features as spatial features, 1: considering some features as spatial features\n",
    "\n",
    "Multicollinearity = 1\n",
    "# 0: without VIF, 1: with variation inflation factor\n",
    "    \n",
    "outliers = 2 \n",
    "# 0: without dummy, 1: dummy variable for all, 2: dummy variables for each feature\n",
    "\n",
    "transformer = 0 \n",
    "# log1p = 0, sqrt = 1, cube root = 2, box-cox = 3\n",
    "\n",
    "numFeatures = 0\n",
    "# 0: considering all features, 1: considering onty top 5 features from coef matrix, 2: considering onty top 10 features  from coef matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"non machinery group dataset.xlsx\")\n",
    "# df.drop(columns='Unnamed: 0', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "idsUnique = len(set(df[\"LandKreis_Name\"]))\n",
    "idsTotal = df.shape[0]\n",
    "idsDupli = idsTotal - idsUnique\n",
    "print(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find most important features relative to target\n",
    "print(\"Find most important features relative to target\")\n",
    "corr = df.corr()\n",
    "corr.sort_values([\"Number_of_machinery\"], ascending = False, inplace = True)\n",
    "corr[\"Number_of_machinery\"].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle remaining missing values for numerical features by using median as replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NAs for numerical features in train : \" + str(df.isnull().values.sum()))\n",
    "# df_num = df_num.fillna(0)\n",
    "# print(\"Remaining NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spatial_regression == 1:\n",
    "    landkreise = geopandas.read_file(\"zip://./geodata/landkreise-in-germany.zip\")\n",
    "\n",
    "    landkreise_feature = landkreise.columns\n",
    "    useless_feature = landkreise_feature.drop(['geometry', 'id_2'])\n",
    "    useless_feature\n",
    "\n",
    "    landkreise.drop(useless_feature, axis=1, inplace=True)\n",
    "    landkreise.rename(columns={'id_2':'ID'}, inplace=True)\n",
    "\n",
    "    df_temp = pd.read_excel('preprocessing_landkreisGeometry.xlsx')\n",
    "    landkreisenGeometry = pd.merge(landkreise, df_temp, on=\"ID\", how='inner')\n",
    "    landkreisenGeometry.drop(labels='LKR-ID', axis=1, inplace=True)\n",
    "    landkreisenGeometry\n",
    "\n",
    "    df = pd.merge(landkreisenGeometry, df, on=\"Landkreis\", how='left')\n",
    "    df = df.drop(['ID'], axis=1)\n",
    "    \n",
    "print(df.shape)\n",
    "print(\"NAs for numerical features in train : \" + str(df.isnull().values.sum()))\n",
    "df = df.fillna(0)\n",
    "print(\"Remaining NAs for numerical features in train : \" + str(df.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiate numerical features (minus the target) and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include = [\"object\"]).columns\n",
    "# categorical_features = categorical_features.drop(['geometry'])\n",
    "numerical_features = df.select_dtypes(exclude = [\"object\"]).columns\n",
    "print(\"Numerical features : \" + str(len(numerical_features)))\n",
    "print(\"Categorical features : \" + str(len(categorical_features)))\n",
    "df_num = df[numerical_features]\n",
    "# fake_num_features = ['LandKreis_ID', 'Company', 'Year']\n",
    "fake_num_features = ['LandKreis_ID', 'Company']\n",
    "df_cat = df['Company']\n",
    "df_num.drop(labels=fake_num_features, axis=1, inplace=True) # LKR-ID is just a classification number\n",
    "print(\"df_cat.shape\", df_cat.shape)\n",
    "print(\"df_num.shape\", df_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of machinery sales correlation matrix\n",
    "corrmat = df_num.corr()\n",
    "k = 11 # number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Number_of_machinery')['Number_of_machinery'].index\n",
    "cm = np.corrcoef(df_num[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "cm = np.corrcoef(df_num[cols].values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "\n",
    "def calculate_vif_(X, thresh=10.0):\n",
    "    variables = list(range(X.shape[1]))\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n",
    "               for ix in range(X.iloc[:, variables].shape[1])]\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n",
    "                  '\\' at index: ' + str(maxloc))\n",
    "            del variables[maxloc]\n",
    "            dropped = True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X.iloc[:, variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of machinery sales correlation matrix\n",
    "corrmat = df_num.corr()\n",
    "k = 11 # number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Number_of_machinery')['Number_of_machinery'].index\n",
    "cm = np.corrcoef(df_num[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "cm = np.corrcoef(df_num[cols].values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_features_label = 'without spatial feature'\n",
    "\n",
    "if spatial_regression == 1:   \n",
    "    \n",
    "    # methology of spatial weight\n",
    "    kW = lp.weights.Kernel.from_dataframe(df, fixed=False, function='gaussian', k=100)\n",
    "    # normalization of spatial weight\n",
    "    kW.transform = 'r'\n",
    "    kW = fill_diagonal(kW, 0)\n",
    "\n",
    "    if spatial_numfeatures == 0:\n",
    "        spatial_features_label = 'Spatial lag for all features'       \n",
    "        spatial_features = df_num.columns \n",
    "        spatial_features = spatial_features.drop([\"Number of machinery\"])\n",
    "    elif spatial_numfeatures == 1:\n",
    "        spatial_features_label = 'Spatial lag for Sum of Number of Tractors, Lw. Betriebe gesamt'\n",
    "        spatial_features = ['Sum of Number of Tractors', 'Lw. Betriebe gesamt']\n",
    "\n",
    "    \n",
    "    X = df[spatial_features]\n",
    "    WX = lp.weights.lag_spatial(kW, X)\n",
    "\n",
    "    WXtable = pd.DataFrame(WX, columns=['lag_{}'.format(name) for name in spatial_features])\n",
    "    df_num = pd.concat((df_num,WXtable),axis=1)\n",
    "    \n",
    "print(df_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with outliers\n",
    "#### https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "### Dealing with outliers by means of Z value\n",
    "# threshold = 5\n",
    "# df_outliners = df_all[(np.abs(stats.zscore(df_all)) < threshold).all(axis=1)]\n",
    "# print('By means of Z-score, we get df shape = {}'.format(df_outliners.shape))\n",
    "\n",
    "### Dealing with outliers by means of Interquartile\n",
    "###Interquartile is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers.\n",
    "# df_IQR = df_num[~((df_num < (Q1 - 1.5 * IQR)) | (df_all > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "# print('getting rid of outliers By means of IQR, we get df shape = {}'.format(df_IQR.shape))\n",
    "# sns.boxplot(x=df_num['Number of machinery'])\n",
    "\n",
    "### Dealing with outliers by means of dummy variable\n",
    "Q1 = df_num.quantile(0.25)\n",
    "Q3 = df_num.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df_dummy = df_num.copy()\n",
    "if outliers == 1:\n",
    "    df_dummy['dummy variable for outliers'] = 0\n",
    "    df_dummy.loc[df_dummy[((df_dummy < (Q1 - 1.5 * IQR)) | \n",
    "                          (df_dummy > (Q3 + 1.5 * IQR))).any(axis=1)].index, ['dummy variable for outliers']] = 1\n",
    "\n",
    "\n",
    "elif outliers == 2:\n",
    "    for column in df_num.columns:\n",
    "        df_dummy['DV for ' + column] = 0\n",
    "        df_dummy.loc[df_dummy[((df_dummy[column] < (Q1[column] - 1.5 * IQR[column])) | \n",
    "                          (df_dummy[column] > (Q3[column] + 1.5 * IQR[column])))].index, ['DV for ' + column]] = 1\n",
    "\n",
    "features = df_num.columns\n",
    "df_dummy.drop(labels=features, axis=1, inplace=True)\n",
    "###\n",
    "print(df_num.shape)\n",
    "print(df_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hstogram\n",
    "sns.distplot(df_num['Number_of_machinery']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform of the skewed numerical features to lessen impact of outliers\n",
    "# As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\n",
    "skewness = df_num.apply(lambda x: skew(x))\n",
    "skewness = skewness[abs(skewness) > 0.5]\n",
    "print(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\n",
    "skewed_features = skewness.index\n",
    "\n",
    "if transformer == 0:\n",
    "    df_num[skewed_features] = np.log1p(df_num[skewed_features]) # 0.65\n",
    "    transformer_label = 'log1p'\n",
    "elif transformer == 1:\n",
    "    df_num[skewed_features] = np.sqrt(df_num[skewed_features]) # 0.53\n",
    "    transformer_label = 'sqrt'\n",
    "elif transformer == 2:\n",
    "    df_num[skewed_features] = df_num[skewed_features]**(1/3)# < 0.62\n",
    "    transformer_label = 'cube root'\n",
    "# lambda = -1. is a reciprocal transform.\n",
    "# lambda = -0.5 is a reciprocal square root transform.\n",
    "# lambda = 0.0 is a log transform.\n",
    "# lambda = 0.5 is a square root transform.\n",
    "# lambda = 1.0 is no transform.\n",
    "elif transformer == 3:\n",
    "    df_num[df_num < 0.001] = 0.001\n",
    "    df_num[skewed_features] = power_transform(df_num[skewed_features], method='box-cox') # 0.6323401820031167\n",
    "    transformer_label = 'box-cox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_num.shape)\n",
    "print(\"NAs for numerical features in train : \" + str(df_num.isnull().values.sum()))\n",
    "df_num = df_num.fillna(0)\n",
    "print(\"Remaining NAs for numerical features in train : \" + str(df_num.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_num.shape)\n",
    "df_num = df_num.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hstogram\n",
    "sns.distplot(df_num['Number_of_machinery']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize numerical features\n",
    "### Scale to [0;1] (give equal weight to all features, assuming a linear distribution of values and no significant outliers)\n",
    "### Shift mean to 0 (to center the data set; this will not affect the output at all for most algorithms.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical features\n",
    "features = df_num.columns\n",
    "stdSc = StandardScaler()\n",
    "df_num = stdSc.fit_transform(df_num)\n",
    "df_num = pd.DataFrame.from_records(df_num)\n",
    "df_num.columns = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collinearity\n",
    "### explicitly explain\n",
    "https://www.youtube.com/watch?v=-2N5aCawArM\n",
    "### source code\n",
    "https://stats.stackexchange.com/questions/155028/how-to-systematically-remove-collinear-variables-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outliers > 0:\n",
    "    df_num = pd.concat([df_num, df_dummy], axis=1)\n",
    "    print(df_num.shape)\n",
    "\n",
    "if Multicollinearity == 0:\n",
    "    Multicollinearity_label = 'without dealing Multicollinearity'\n",
    "elif Multicollinearity == 1:\n",
    "    df_num = calculate_vif_(df_num)\n",
    "    Multicollinearity_label = 'variance inflation factor'\n",
    "print(df_num.shape)\n",
    "\n",
    "# number of machinery sales correlation matrix\n",
    "corrmat = df_num.corr()\n",
    "k = 11 # number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Number_of_machinery')['Number_of_machinery'].index\n",
    "cm = np.corrcoef(df_num[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "cm = np.corrcoef(df_num[cols].values.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-effect model preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed_effect_model_random-intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new data frame with one hot encoded columns for the counties\n",
    "if effect_model == 1:\n",
    "    df_all = pd.concat([df_num, df_cat], axis=1)\n",
    "    figsize = (30, 50)\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=2, ncols=1)\n",
    "    top10 = [\"Number_of_machinery\"]\n",
    "    for i, top in enumerate(top10):\n",
    "        sns.catplot(x=\"Company\", y=top, data=df_all, kind = \"swarm\", ax=axes[i])\n",
    "        plt.close(2)\n",
    "    # one hot coding for company\n",
    "    Company = df_all.Company.unique()\n",
    "    df_num = pd.concat([df_num, pd.get_dummies(df_all.Company)],axis=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-effect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge statsmodels -y\n",
    "if effect_model == 2:\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "    # construct our model, with our county now shown as a group\n",
    "    md = smf.mixedlm(\"Q('Number of machinery') ~ \\\n",
    "                     Q('Fläche [ha]') + \\\n",
    "                     Ackerbaubetriebe + \\\n",
    "                     Q('Betriebe 20-50 ha') + \\\n",
    "                     Q('LW-Fläche 50-100 ha') + \\\n",
    "                     Q('LW-Fläche 50-100 ha')\",\n",
    "                     df_all, groups=df_all[\"Company\"])\n",
    "    mdf = md.fit()\n",
    "    print(mdf.summary())\n",
    "    # and let's store the rmse\n",
    "    from math import sqrt\n",
    "    y_predict = mdf.fittedvalues\n",
    "    RMSE = sqrt(((df_all['Number of machinery']-y_predict)**2).values.mean())\n",
    "    results = pd.DataFrame()\n",
    "    results[\"Method\"] = [\"Mixed\"]\n",
    "    results[\"RMSE\"] = RMSE\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# differentiate dependent and indenpendent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_num[\"Number_of_machinery\"]\n",
    "y_features = df_num.columns\n",
    "y_features = y_features.drop(\"Number_of_machinery\")\n",
    "df_num = df_num[y_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if numFeatures == 1:\n",
    "    nonfive_features = df_num.columns\n",
    "    top5 = [\"Fläche [ha]\", \"Ackerbaubetriebe\", \"Betriebe 20-50 ha\", \"LW-Fläche 20-50 ha\", 'LW-Fläche 50-100 ha']\n",
    "    nonfive_features = nonfive_features.drop(top5)\n",
    "    df_num.drop(labels=nonfive_features, axis=1, inplace=True)\n",
    "elif numFeatures == 2:\n",
    "    nonten_features = df_num.columns\n",
    "    top10 = [\"Company\", \"Fläche [ha]\", \"Ackerbaubetriebe\", \"Betriebe 20-50 ha\", \"LW-Fläche 20-50 ha\",\n",
    "     \"LW-Fläche 50-100 ha\", \"101-200\", \"Betriebe Rinder\", \"Grünlandfläche [ha]\", \"Betriebe 50-100 ha\"]\n",
    "    nonten_features = nonten_features.drop(top10)\n",
    "    df_num.drop(labels=nonten_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join categorical and numerical features\n",
    "# train = pd.concat([df_num, df_cat], axis = 1)\n",
    "train = df_num\n",
    "print(\"New number of features : \" + str(train.shape[1]))\n",
    "\n",
    "# Partition the dataset in train + validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.25, random_state = 0)\n",
    "print(\"X_train : \" + str(X_train.shape))\n",
    "print(\"X_test : \" + str(X_test.shape))\n",
    "print(\"y_train : \" + str(y_train.shape))\n",
    "print(\"y_test : \" + str(y_test.shape))\n",
    "\n",
    "# Define error measure for official scoring : RMSE\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "\n",
    "def rmse_cv_train(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10)) # k-fold, set k as 10\n",
    "    return(rmse)\n",
    "\n",
    "def rmse_cv_test(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1* Linear Regression without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Look at predictions on training and validation set\n",
    "# printing values\n",
    "\n",
    "    \n",
    "if linear_model_type == 0:\n",
    "    linear_model_type_label = 'LinearRegression'\n",
    "    print(linear_model_type_label)\n",
    "elif linear_model_type == 1:\n",
    "    linear_model_type_label = 'RANSACRegression'\n",
    "    print(linear_model_type_label)\n",
    "elif linear_model_type == 2:\n",
    "    linear_model_type_label = 'LinearRegression_Ridge'\n",
    "    print(linear_model_type_label)\n",
    "elif linear_model_type == 3:\n",
    "    linear_model_type_label = 'LinearRegression_Lasso'\n",
    "    print(linear_model_type_label)\n",
    "elif linear_model_type == 4:\n",
    "    linear_model_type_label = 'LinearRegression_ElasticNet'\n",
    "    print(linear_model_type_label)\n",
    "        \n",
    "if spatial_regression == 0:\n",
    "    spatial_regression_label = 'No_SpatialRegression'\n",
    "    print(spatial_regression_label)\n",
    "elif spatial_regression == 1:\n",
    "    spatial_regression_label = 'SpatialRegression'\n",
    "    print(spatial_regression_label)\n",
    "\n",
    "if outliers == 0:\n",
    "    outliers_label = 'without dummy'\n",
    "    print(outliers_label)\n",
    "elif outliers == 1:\n",
    "    outliers_label = 'Dummy for all features'\n",
    "    print(outliers_label)\n",
    "elif outliers == 2:\n",
    "    outliers_label = 'Dummy for each feature'\n",
    "    print(outliers_label)\n",
    "\n",
    "if numFeatures == 0:\n",
    "    numFeatures_label = 'all features'   \n",
    "    print(numFeatures_label)\n",
    "elif numFeatures == 1:\n",
    "    numFeatures_label = 'Top 5 features'\n",
    "    print(numFeatures_label)\n",
    "elif numFeatures == 2:\n",
    "    numFeatures_label = 'Top 10 features'\n",
    "    print(numFeatures_label)\n",
    "    \n",
    "print(transformer_label)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "X2 = sm.add_constant(X_train)\n",
    "est = sm.OLS(y_train, X2)\n",
    "est2 = est.fit()\n",
    "est2.save(\"random-intercept_results.txt\")\n",
    "print(est2.summary())\n",
    "\n",
    "# Linear Regression\n",
    "if linear_model_type == 0:\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_coef = list(zip(lr.coef_, df_num.columns))\n",
    "    lr_coef.sort(reverse=True)\n",
    "    intercept_ = lr.intercept_\n",
    "    print(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\n",
    "    print(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\n",
    "    y_train_pred = lr.predict(X_train)\n",
    "    y_test_pred = lr.predict(X_test)\n",
    "    df_lr_coef = pd.DataFrame.from_records(lr_coef, columns=['coef', 'features'])\n",
    "elif linear_model_type == 1:\n",
    "    lr = RANSACRegressor()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_coef = list(zip(lr.estimator_.coef_, df_num.columns))\n",
    "    lr_coef.sort(reverse=True)\n",
    "    intercept_ = lr.estimator_.intercept_\n",
    "    print(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\n",
    "    print(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\n",
    "    y_train_pred = lr.predict(X_train)\n",
    "    y_test_pred = lr.predict(X_test)\n",
    "    df_lr_coef = pd.DataFrame.from_records(lr_coef, columns=['coef', 'features'])\n",
    "\n",
    "# 2* Ridge\n",
    "elif linear_model_type == 2:\n",
    "    ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\n",
    "    ridge.fit(X_train, y_train)\n",
    "    alpha = ridge.alpha_\n",
    "    print(\"Best alpha :\", alpha)\n",
    "\n",
    "    print(\"Try again for more precision with alphas centered around \" + str(alpha))\n",
    "    ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n",
    "                              alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n",
    "                              alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n",
    "                    cv = 10)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    alpha = ridge.alpha_\n",
    "    print(\"Best alpha :\", alpha)\n",
    "\n",
    "    print(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\n",
    "    print(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\n",
    "    y_train_rdg = ridge.predict(X_train)\n",
    "    y_test_rdg = ridge.predict(X_test)\n",
    "\n",
    "    y_train_pred = ridge.predict(X_train)\n",
    "    y_test_pred = ridge.predict(X_test)\n",
    "\n",
    "    ridge_coef = list(zip(ridge.coef_, df_num.columns))\n",
    "    ridge_coef.sort(reverse=True)\n",
    "    df_ridge_coef = pd.DataFrame.from_records(ridge_coef, columns=['coef', 'features'])\n",
    "    intercept_ = ridge.intercept_\n",
    "    df_lr_coef =  df_ridge_coef\n",
    "# 3* Lasso\n",
    "elif linear_model_type == 3:\n",
    "\n",
    "    lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n",
    "                              0.3, 0.6, 1], \n",
    "                    max_iter = 50000, cv = 10)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    alpha = lasso.alpha_\n",
    "    print(\"Best alpha :\", alpha)\n",
    "\n",
    "    print(\"Try again for more precision with alphas centered around \" + str(alpha))\n",
    "    lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n",
    "                              alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n",
    "                              alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n",
    "                              alpha * 1.4], \n",
    "                    max_iter = 50000, cv = 10)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    alpha = lasso.alpha_\n",
    "    print(\"Best alpha :\", alpha)\n",
    "\n",
    "    print(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\n",
    "    print(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\n",
    "    y_train_las = lasso.predict(X_train)\n",
    "    y_test_las = lasso.predict(X_test)\n",
    "\n",
    "    y_train_pred = lasso.predict(X_train)\n",
    "    y_test_pred = lasso.predict(X_test)\n",
    "\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    lasso_coef = list(zip(lasso.coef_, df_num.columns))\n",
    "    lasso_coef.sort(reverse=True)\n",
    "    df_lasso_coef = pd.DataFrame.from_records(lasso_coef, columns=['coef', 'features'])\n",
    "    intercept_ = lasso.intercept_\n",
    "    df_lr_coef =  df_lasso_coef\n",
    "# 4* ElasticNet    \n",
    "elif linear_model_type == 4:  \n",
    "    elasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n",
    "                              alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n",
    "                                        0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n",
    "                              max_iter = 50000, cv = 10)\n",
    "    elasticNet.fit(X_train, y_train)\n",
    "    alpha = elasticNet.alpha_\n",
    "    ratio = elasticNet.l1_ratio_\n",
    "    print(\"Best l1_ratio :\", ratio)\n",
    "    print(\"Best alpha :\", alpha )\n",
    "\n",
    "    print(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\n",
    "    elasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n",
    "                              alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n",
    "                              max_iter = 50000, cv = 10)\n",
    "    elasticNet.fit(X_train, y_train)\n",
    "    if (elasticNet.l1_ratio_ > 1):\n",
    "        elasticNet.l1_ratio_ = 1    \n",
    "    alpha = elasticNet.alpha_\n",
    "    ratio = elasticNet.l1_ratio_\n",
    "    print(\"Best l1_ratio :\", ratio)\n",
    "    print(\"Best alpha :\", alpha )\n",
    "\n",
    "    print(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n",
    "          \" and alpha centered around \" + str(alpha))\n",
    "    elasticNet = ElasticNetCV(l1_ratio = ratio,\n",
    "                              alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n",
    "                                        alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n",
    "                                        alpha * 1.35, alpha * 1.4], \n",
    "                              max_iter = 50000, cv = 10)\n",
    "    elasticNet.fit(X_train, y_train)\n",
    "    if (elasticNet.l1_ratio_ > 1):\n",
    "        elasticNet.l1_ratio_ = 1    \n",
    "    alpha = elasticNet.alpha_\n",
    "    ratio = elasticNet.l1_ratio_\n",
    "    print(\"Best l1_ratio :\", ratio)\n",
    "    print(\"Best alpha :\", alpha )\n",
    "\n",
    "    print(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\n",
    "    print(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\n",
    "    y_train_pred = elasticNet.predict(X_train)\n",
    "    y_test_pred = elasticNet.predict(X_test)\n",
    "\n",
    "    elasticNet_coef = list(zip(elasticNet.coef_, df_num.columns))\n",
    "    elasticNet_coef.sort(reverse=True)\n",
    "    df_elasticNet_coef = pd.DataFrame.from_records(elasticNet_coef, columns=['coef', 'features'])\n",
    "    intercept_ = elasticNet.intercept_\n",
    "    df_lr_coef = df_elasticNet_coef\n",
    "#############\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "\n",
    "print('Intercept:', intercept_)\n",
    "print('R2 score of train: ', r2_train)\n",
    "print('R2 score of test: ', r2_test)\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
    "plt.title(\"Linear regression\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.hlines(y = 0, xmin = 0, xmax = 8, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions\n",
    "plt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "plt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
    "plt.title(\"Linear regression\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Real values\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.plot([0, 8], [0, 8], c = \"red\")\n",
    "plt.show()\n",
    "\n",
    "top10_coef_features = []\n",
    "for feature in df_lr_coef.nlargest(k-1, 'coef').features:\n",
    "    top10_coef_features.append(feature)\n",
    "print(\"top10_coef_features\", top10_coef_features)\n",
    "\n",
    "top10_corr_features = []\n",
    "for feature in cols:\n",
    "    if feature == 'Number of machinery':\n",
    "        continue\n",
    "    top10_corr_features.append(feature)\n",
    "print(\"top10_corr_features\", top10_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr_coef.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "sns.set()\n",
    "cols = top10_coef_features\n",
    "sns.pairplot(df_num[cols])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scatterplot\n",
    "# sns.set()\n",
    "# cols = top10_corr_features\n",
    "# sns.pairplot(df_num[cols])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts in Spatial Linear Modelling\n",
    "\n",
    "### Data Borrowing in Supervised Learning\n",
    "# Kernel Regressions\n",
    "Kernel regressions are one exceptionally common way to allow observations to \"borrow strength\" from nearby observations. \n",
    "\n",
    "However, when working with spatial data, there are *two simultaneous senses of what is near:* \n",
    "- things that similar in attribute (classical kernel regression)\n",
    "- things that are similar in spatial position (spatial kernel regression)\n",
    "\n",
    "Below, we'll walk through how to use scikit to fit these two types of kernel regressions, show how it's not super simple to mix the two approaches together, and refer to an approach that does this correctly in another package. \n",
    "\n",
    "One method that can exploit the fact that local data may be more informative in predicting $y$ at site $i$ than distant data is Geographically Weighted Regression, a type of Generalized Additive Spatial Model. Kind of like a Kernel Regression, GWR conducts a bunch of regressions at each training site only considering data near that site. This means it works like the kernel regressions above, but uses *both* the coordinates *and* the data in $X$ to predict $y$ at each site. It optimizes its sense of \"local\" depending on some information criteria or fit score.\n",
    "\n",
    "You can find this in the `gwr` package, and significant development is ongoing on this at `https://github.com/pysal/gwr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "# construct our model, with our county now shown as a group\n",
    "md = smf.mixedlm(\"Number_of_machinery ~ \\\n",
    "                   Betriebe_Zuchtsauen + \\\n",
    "                  Betriebe_50_100_ha + \\\n",
    "                  Power_101_200 + \\\n",
    "                  Ackerbaubetriebe + \\\n",
    "                  Anzahl_Zuchtsauen+ \\\n",
    "                 Viehbestand_GV + \\\n",
    "                Anzahl_Schweine + \\\n",
    "                  Q('Fläche_Triticale_[ha]') + \\\n",
    "                 Number_of_Tractors + \\\n",
    "                 Q('Silomais_[ha]') + \\\n",
    "                 Q('Körnermais_[ha]') + \\\n",
    "                 Bestand_Rinder + \\\n",
    "                  Q('Kartoffeln_[ha]')\",\n",
    "                 df_all, groups=df_all[\"Company\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())\n",
    "# and let's store the rmse\n",
    "from math import sqrt\n",
    "y_predict = mdf.fittedvalues\n",
    "RMSE = sqrt(((df_all['Number_of_machinery']-y_predict)**2).values.mean())\n",
    "results = pd.DataFrame()\n",
    "results[\"Method\"] = [\"Mixed\"]\n",
    "results[\"RMSE\"] = RMSE\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and let's plot the predictions\n",
    "performance = pd.DataFrame()\n",
    "performance = df_all\n",
    "performance[\"residuals\"] = mdf.resid.values\n",
    "performance[\"predicted\"] = mdf.fittedvalues\n",
    "\n",
    "sns.lmplot(x = \"predicted\", y = \"residuals\", data = performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct our model, with our county now shown as a group\n",
    "md = smf.mixedlm(\"Number_of_machinery ~ \\\n",
    "                   Betriebe_Zuchtsauen + \\\n",
    "                  Betriebe_50_100_ha + \\\n",
    "                  P_101_200 + \\\n",
    "                  Ackerbaubetriebe + \\\n",
    "                  Anzahl_Zuchtsauen+ \\\n",
    "                 Viehbestand_GV + \\\n",
    "                Anzahl_Schweine + \\\n",
    "                  Q('Fläche_Triticale_[ha]') + \\\n",
    "                 Number_of_Tractors + \\\n",
    "                 Q('Silomais_[ha]') + \\\n",
    "                 Q('Körnermais_[ha]') + \\\n",
    "                 Bestand_Rinder + \\\n",
    "                  Q('Kartoffeln_[ha]')\",\n",
    "                 df_all, groups=df_all[\"Company\"], re_formula=\"~Number_of_machinery\")\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())\n",
    "# and let's store the rmse\n",
    "from math import sqrt\n",
    "y_predict = mdf.fittedvalues\n",
    "RMSE = sqrt(((df_all['Number_of_machinery']-y_predict)**2).values.mean())\n",
    "results = pd.DataFrame()\n",
    "results[\"Method\"] = [\"Mixed\"]\n",
    "results[\"RMSE\"] = RMSE\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R install.packages(c('lme4','lmerTest','lsmeans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R library(lme4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R install.packages(\"readxl\")\n",
    "%R library(\"readxl\")\n",
    "# xlsx files\n",
    "%R df <- read_excel('All_Data_AllTractors.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "\n",
    "r = robjects.r\n",
    "lm_D90 = r.lmer('Number_of_machinery ~ Betriebe_Zuchtsauen + P_101_200 + (1|Company)', data=df_all, REML=False)\n",
    "print(r.summary(lm_D90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmer = r.lmer('Number_of_machinery ~ Betriebe_Zuchtsauen + P_101_200 + (1+Betriebe_Zuchtsauen|Company)', data=df_all, REML=False)\n",
    "print(r.summary(lmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.coef(lmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R pi\n",
    "%R pitch = c(233,204,242,130,112,142)\n",
    "# %R sex = c(rep(\"female\",3),rep(\"male\",3))\n",
    "%R sex = c(233,204,242,130,112,142)\n",
    "%R my.df = data.frame(sex,pitch)\n",
    "%R xmdl = lm(pitch ~ sex, my.df)\n",
    "%R summary(xmdl)\n",
    "%R print(type(my.df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
