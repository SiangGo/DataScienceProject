{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , power_transform#\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, RANSACRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "from scipy.stats import skew\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as skn\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "import libpysal as lp\n",
    "import geopandas\n",
    "import shapely.geometry as shp\n",
    "\n",
    "from libpysal.weights.util import fill_diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_model = 0\n",
    "# No_fixed_effect_model = 0, fixed_effect_model = 1\n",
    "\n",
    "linear_model_type = 0\n",
    "# LinearRegression = 0, RANSACRegressor = 1, LinearRegression_Ridge = 2, LinearRegression_Ridge = 3, LinearRegression_Elastic = 4\n",
    "\n",
    "spatial_regression = 0 \n",
    "# without Spatial Regression = 0, Spatial Regression = 1\n",
    "\n",
    "spatial_numfeatures = 0\n",
    "# 0: considering all features as spatial features, 1: considering some features as spatial features\n",
    "\n",
    "Multicollinearity = 1\n",
    "# 0: without Multicollinearity, 1: with Multicollinearity\n",
    "    \n",
    "outliers = 0 \n",
    "# 0: without dummy, 1: dummy variable for all, 2: dummy variables for each feature\n",
    "\n",
    "transformer = 0 \n",
    "# log1p = 0, sqrt = 1, cube root = 2, box-cox = 3\n",
    "\n",
    "numFeatures = 0\n",
    "# 0: considering all features, 1: considering onty top 5 features from coef matrix, 2: considering onty top 10 features  from coef matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(401, 59)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LandKreis ID</th>\n",
       "      <th>Landkreis</th>\n",
       "      <th>KFZ</th>\n",
       "      <th>Fläche [ha]</th>\n",
       "      <th>LF gesamt [ha]</th>\n",
       "      <th>Anteil LF [%]</th>\n",
       "      <th>Lw. Betriebe gesamt</th>\n",
       "      <th>Betriebe bis 5 ha</th>\n",
       "      <th>Betriebe 5-20 ha</th>\n",
       "      <th>Betriebe 20-50 ha</th>\n",
       "      <th>...</th>\n",
       "      <th>Betriebe Öko-LF in Umst.</th>\n",
       "      <th>LF ökologisch [ha]</th>\n",
       "      <th>LW-Fläche unter 5 ha</th>\n",
       "      <th>LW-Fläche 10-20 ha</th>\n",
       "      <th>LW-Fläche 20-50 ha</th>\n",
       "      <th>LW-Fläche 50-100 ha</th>\n",
       "      <th>LW-Fläche 100-200 ha</th>\n",
       "      <th>LW-Fläche über 200 ha</th>\n",
       "      <th>Number of machinery</th>\n",
       "      <th>Sum of Number of Tractors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7140</td>\n",
       "      <td>LK Rhein-Hunsrück-Kreis</td>\n",
       "      <td>SIM</td>\n",
       "      <td>93414.311</td>\n",
       "      <td>37301</td>\n",
       "      <td>39.900</td>\n",
       "      <td>719</td>\n",
       "      <td>31</td>\n",
       "      <td>300</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>1913</td>\n",
       "      <td>61</td>\n",
       "      <td>3377</td>\n",
       "      <td>4979</td>\n",
       "      <td>9330</td>\n",
       "      <td>10797</td>\n",
       "      <td>8757</td>\n",
       "      <td>34</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7141</td>\n",
       "      <td>LK Rhein-Lahn-Kreis</td>\n",
       "      <td>EMS</td>\n",
       "      <td>75847.110</td>\n",
       "      <td>26679</td>\n",
       "      <td>35.200</td>\n",
       "      <td>491</td>\n",
       "      <td>31</td>\n",
       "      <td>178</td>\n",
       "      <td>92</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>1813</td>\n",
       "      <td>69</td>\n",
       "      <td>1931</td>\n",
       "      <td>3068</td>\n",
       "      <td>7318</td>\n",
       "      <td>9321</td>\n",
       "      <td>4973</td>\n",
       "      <td>38</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7143</td>\n",
       "      <td>LK Westerwaldkreis</td>\n",
       "      <td>WW</td>\n",
       "      <td>95960.505</td>\n",
       "      <td>26296</td>\n",
       "      <td>27.400</td>\n",
       "      <td>503</td>\n",
       "      <td>26</td>\n",
       "      <td>196</td>\n",
       "      <td>115</td>\n",
       "      <td>...</td>\n",
       "      <td>452</td>\n",
       "      <td>6862</td>\n",
       "      <td>80</td>\n",
       "      <td>2358</td>\n",
       "      <td>3816</td>\n",
       "      <td>6100</td>\n",
       "      <td>8649</td>\n",
       "      <td>5294</td>\n",
       "      <td>2</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7211</td>\n",
       "      <td>SK Trier</td>\n",
       "      <td>TR</td>\n",
       "      <td>11376.318</td>\n",
       "      <td>1860</td>\n",
       "      <td>16.300</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>121</td>\n",
       "      <td>394</td>\n",
       "      <td>523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7231</td>\n",
       "      <td>LK Bernkastel-Wittlich</td>\n",
       "      <td>WIL</td>\n",
       "      <td>113297.133</td>\n",
       "      <td>34098</td>\n",
       "      <td>30.100</td>\n",
       "      <td>1319</td>\n",
       "      <td>645</td>\n",
       "      <td>342</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>226</td>\n",
       "      <td>2728</td>\n",
       "      <td>1449</td>\n",
       "      <td>3417</td>\n",
       "      <td>4083</td>\n",
       "      <td>7615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LandKreis ID                Landkreis  KFZ  Fläche [ha]  LF gesamt [ha]  \\\n",
       "0          7140  LK Rhein-Hunsrück-Kreis  SIM    93414.311           37301   \n",
       "1          7141      LK Rhein-Lahn-Kreis  EMS    75847.110           26679   \n",
       "2          7143       LK Westerwaldkreis   WW    95960.505           26296   \n",
       "3          7211                 SK Trier   TR    11376.318            1860   \n",
       "4          7231   LK Bernkastel-Wittlich  WIL   113297.133           34098   \n",
       "\n",
       "   Anteil LF [%]  Lw. Betriebe gesamt  Betriebe bis 5 ha  Betriebe 5-20 ha  \\\n",
       "0         39.900                  719                 31               300   \n",
       "1         35.200                  491                 31               178   \n",
       "2         27.400                  503                 26               196   \n",
       "3         16.300                   50                 12                13   \n",
       "4         30.100                 1319                645               342   \n",
       "\n",
       "   Betriebe 20-50 ha            ...              Betriebe Öko-LF in Umst.  \\\n",
       "0                150            ...                                    25   \n",
       "1                 92            ...                                   180   \n",
       "2                115            ...                                   452   \n",
       "3                 13            ...                                     0   \n",
       "4                129            ...                                   226   \n",
       "\n",
       "   LF ökologisch [ha]  LW-Fläche unter 5 ha  LW-Fläche 10-20 ha  \\\n",
       "0                1913                    61                3377   \n",
       "1                1813                    69                1931   \n",
       "2                6862                    80                2358   \n",
       "3                  74                    23                 121   \n",
       "4                2728                  1449                3417   \n",
       "\n",
       "   LW-Fläche 20-50 ha  LW-Fläche 50-100 ha  LW-Fläche 100-200 ha  \\\n",
       "0                4979                 9330                 10797   \n",
       "1                3068                 7318                  9321   \n",
       "2                3816                 6100                  8649   \n",
       "3                 394                  523                     0   \n",
       "4                4083                 7615                     0   \n",
       "\n",
       "   LW-Fläche über 200 ha  Number of machinery  Sum of Number of Tractors  \n",
       "0                   8757                   34                        497  \n",
       "1                   4973                   38                        450  \n",
       "2                   5294                    2                        751  \n",
       "3                      0                    1                        216  \n",
       "4                      0                   25                        920  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"df_Merge_ASDnVehiclenCompany_noMissingCol.xlsx\")\n",
    "# df.drop(columns='Unnamed: 0', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicate IDs for 401 total entries\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "idsUnique = len(set(df[\"LandKreis ID\"]))\n",
    "idsTotal = df.shape[0]\n",
    "idsDupli = idsTotal - idsUnique\n",
    "print(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find most important features relative to target\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Number of machinery         1.000\n",
       "Betriebe Zuchtsauen         0.693\n",
       "Ackerbaubetriebe            0.692\n",
       "Betriebe 50-100 ha          0.686\n",
       "LW-Fläche 50-100 ha         0.684\n",
       "Betriebe Schweine           0.667\n",
       "Sum of Number of Tractors   0.628\n",
       "Viehbestand GV              0.626\n",
       "Anzahl Zuchtsauen           0.616\n",
       "Betriebe Viehhaltung        0.606\n",
       "Lw. Betriebe gesamt         0.604\n",
       "Anzahl Schweine             0.601\n",
       "Fläche Triticale [ha]       0.590\n",
       "LW-Fläche 20-50 ha          0.581\n",
       "Silomais [ha]               0.574\n",
       "Betriebe 20-50 ha           0.573\n",
       "Fläche Grünernte [ha]       0.565\n",
       "Körnermais [ha]             0.515\n",
       "Betriebe 100-200 ha         0.509\n",
       "Bestand Rinder              0.506\n",
       "Name: Number of machinery, dtype: float64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most important features relative to target\n",
    "print(\"Find most important features relative to target\")\n",
    "corr = df.corr()\n",
    "corr.sort_values([\"Number of machinery\"], ascending = False, inplace = True)\n",
    "corr[\"Number of machinery\"].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle remaining missing values for numerical features by using median as replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAs for numerical features in train : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NAs for numerical features in train : \" + str(df.isnull().values.sum()))\n",
    "# df_num = df_num.fillna(0)\n",
    "# print(\"Remaining NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(401, 59)\n",
      "NAs for numerical features in train : 0\n",
      "Remaining NAs for numerical features in train : 0\n"
     ]
    }
   ],
   "source": [
    "if spatial_regression == 1:\n",
    "    landkreise = geopandas.read_file(\"zip://./geodata/landkreise-in-germany.zip\")\n",
    "\n",
    "    landkreise_feature = landkreise.columns\n",
    "    useless_feature = landkreise_feature.drop(['geometry', 'id_2'])\n",
    "    useless_feature\n",
    "\n",
    "    landkreise.drop(useless_feature, axis=1, inplace=True)\n",
    "    landkreise.rename(columns={'id_2':'ID'}, inplace=True)\n",
    "\n",
    "    df_temp = pd.read_excel('preprocessing_landkreisGeometry.xlsx')\n",
    "    landkreisenGeometry = pd.merge(landkreise, df_temp, on=\"ID\", how='inner')\n",
    "    landkreisenGeometry.drop(labels='LKR-ID', axis=1, inplace=True)\n",
    "    landkreisenGeometry\n",
    "\n",
    "    df = pd.merge(landkreisenGeometry, df, on=\"Landkreis\", how='left')\n",
    "    df = df.drop(['ID'], axis=1)\n",
    "    \n",
    "print(df.shape)\n",
    "print(\"NAs for numerical features in train : \" + str(df.isnull().values.sum()))\n",
    "df = df.fillna(0)\n",
    "print(\"Remaining NAs for numerical features in train : \" + str(df.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiate numerical features (minus the target) and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features : 57\n",
      "Categorical features : 2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Company'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda\\Setup\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2133\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2134\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2135\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4433)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4279)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13742)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13696)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Company'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-eb8bd655f062>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumerical_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfake_num_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'LandKreis ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Company'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Company'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdf_num\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfake_num_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# LKR-ID is just a classification number\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df_cat.shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_cat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\Setup\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2057\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2059\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\Setup\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2064\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2066\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\Setup\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1384\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1386\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1387\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\Setup\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3542\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3543\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3544\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3545\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\Setup\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2134\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2136\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4433)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4279)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13742)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13696)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Company'"
     ]
    }
   ],
   "source": [
    "categorical_features = df.select_dtypes(include = [\"object\"]).columns\n",
    "# categorical_features = categorical_features.drop(['geometry'])\n",
    "numerical_features = df.select_dtypes(exclude = [\"object\"]).columns\n",
    "print(\"Numerical features : \" + str(len(numerical_features)))\n",
    "print(\"Categorical features : \" + str(len(categorical_features)))\n",
    "df_num = df[numerical_features]\n",
    "fake_num_features = ['LandKreis ID', 'Company', 'Year']\n",
    "df_cat = df['Company']\n",
    "df_num.drop(labels=fake_num_features, axis=1, inplace=True) # LKR-ID is just a classification number\n",
    "print(\"df_cat.shape\", df_cat.shape)\n",
    "print(\"df_num.shape\", df_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of machinery sales correlation matrix\n",
    "corrmat = df_num.corr()\n",
    "k = 11 # number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Number of machinery')['Number of machinery'].index\n",
    "cm = np.corrcoef(df_num[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "cm = np.corrcoef(df_num[cols].values.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collinearity\n",
    "### explicitly explain\n",
    "https://www.youtube.com/watch?v=-2N5aCawArM\n",
    "### source code\n",
    "https://stats.stackexchange.com/questions/155028/how-to-systematically-remove-collinear-variables-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "\n",
    "def calculate_vif_(X, thresh=5.0):\n",
    "    variables = list(range(X.shape[1]))\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n",
    "               for ix in range(X.iloc[:, variables].shape[1])]\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n",
    "                  '\\' at index: ' + str(maxloc))\n",
    "            del variables[maxloc]\n",
    "            dropped = True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X.iloc[:, variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Multicollinearity == 0:\n",
    "    Multicollinearity_label = 'without dealing Multicollinearity'\n",
    "elif Multicollinearity == 1:\n",
    "    df_num = calculate_vif_(df_num)\n",
    "    Multicollinearity_label = 'variance inflation factor'\n",
    "print(df_num.shape)\n",
    "\n",
    "# number of machinery sales correlation matrix\n",
    "corrmat = df_num.corr()\n",
    "k = 11 # number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Number of machinery')['Number of machinery'].index\n",
    "cm = np.corrcoef(df_num[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "cm = np.corrcoef(df_num[cols].values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_features_label = 'without spatial feature'\n",
    "\n",
    "if spatial_regression == 1:   \n",
    "    \n",
    "    # methology of spatial weight\n",
    "    kW = lp.weights.Kernel.from_dataframe(df, fixed=False, function='gaussian', k=100)\n",
    "    # normalization of spatial weight\n",
    "    kW.transform = 'r'\n",
    "    kW = fill_diagonal(kW, 0)\n",
    "\n",
    "    if spatial_numfeatures == 0:\n",
    "        spatial_features_label = 'Spatial lag for all features'       \n",
    "        spatial_features = df_num.columns \n",
    "        spatial_features = spatial_features.drop([\"Number of machinery\"])\n",
    "    elif spatial_numfeatures == 1:\n",
    "        spatial_features_label = 'Spatial lag for Sum of Number of Tractors, Lw. Betriebe gesamt'\n",
    "        spatial_features = ['Sum of Number of Tractors', 'Lw. Betriebe gesamt']\n",
    "\n",
    "    \n",
    "    X = df[spatial_features]\n",
    "    WX = lp.weights.lag_spatial(kW, X)\n",
    "\n",
    "    WXtable = pd.DataFrame(WX, columns=['lag_{}'.format(name) for name in spatial_features])\n",
    "    df_num = pd.concat((df_num,WXtable),axis=1)\n",
    "    \n",
    "print(df_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with outliers\n",
    "#### https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "### Dealing with outliers by means of Z value\n",
    "# threshold = 5\n",
    "# df_outliners = df_all[(np.abs(stats.zscore(df_all)) < threshold).all(axis=1)]\n",
    "# print('By means of Z-score, we get df shape = {}'.format(df_outliners.shape))\n",
    "\n",
    "### Dealing with outliers by means of Interquartile\n",
    "###Interquartile is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers.\n",
    "# df_IQR = df_num[~((df_num < (Q1 - 1.5 * IQR)) | (df_all > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "# print('getting rid of outliers By means of IQR, we get df shape = {}'.format(df_IQR.shape))\n",
    "# sns.boxplot(x=df_num['Number of machinery'])\n",
    "\n",
    "### Dealing with outliers by means of dummy variable\n",
    "Q1 = df_num.quantile(0.25)\n",
    "Q3 = df_num.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df_dummy = df_num.copy()\n",
    "if outliers == 1:\n",
    "    df_dummy['dummy variable for outliers'] = 0\n",
    "    df_dummy.loc[df_dummy[((df_dummy < (Q1 - 1.5 * IQR)) | \n",
    "                          (df_dummy > (Q3 + 1.5 * IQR))).any(axis=1)].index, ['dummy variable for outliers']] = 1\n",
    "\n",
    "\n",
    "elif outliers == 2:\n",
    "    for column in df_num.columns:\n",
    "        df_dummy['DV for ' + column] = 0\n",
    "        df_dummy.loc[df_dummy[((df_dummy[column] < (Q1[column] - 1.5 * IQR[column])) | \n",
    "                          (df_dummy[column] > (Q3[column] + 1.5 * IQR[column])))].index, ['DV for ' + column]] = 1\n",
    "\n",
    "features = df_num.columns\n",
    "df_dummy.drop(labels=features, axis=1, inplace=True)\n",
    "###\n",
    "print(df_num.shape)\n",
    "print(df_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hstogram\n",
    "sns.distplot(df_num['Number of machinery']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform of the skewed numerical features to lessen impact of outliers\n",
    "# As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\n",
    "skewness = df_num.apply(lambda x: skew(x))\n",
    "skewness = skewness[abs(skewness) > 0.5]\n",
    "print(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\n",
    "skewed_features = skewness.index\n",
    "\n",
    "if transformer == 0:\n",
    "    df_num[skewed_features] = np.log1p(df_num[skewed_features]) # 0.65\n",
    "    transformer_label = 'log1p'\n",
    "elif transformer == 1:\n",
    "    df_num[skewed_features] = np.sqrt(df_num[skewed_features]) # 0.53\n",
    "    transformer_label = 'sqrt'\n",
    "elif transformer == 2:\n",
    "    df_num[skewed_features] = df_num[skewed_features]**(1/3)# < 0.62\n",
    "    transformer_label = 'cube root'\n",
    "# lambda = -1. is a reciprocal transform.\n",
    "# lambda = -0.5 is a reciprocal square root transform.\n",
    "# lambda = 0.0 is a log transform.\n",
    "# lambda = 0.5 is a square root transform.\n",
    "# lambda = 1.0 is no transform.\n",
    "elif transformer == 3:\n",
    "    df_num[df_num < 0.001] = 0.001\n",
    "    df_num[skewed_features] = power_transform(df_num[skewed_features], method='box-cox') # 0.6323401820031167\n",
    "    transformer_label = 'box-cox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_num.shape)\n",
    "print(\"NAs for numerical features in train : \" + str(df_num.isnull().values.sum()))\n",
    "df_num = df_num.fillna(0)\n",
    "print(\"Remaining NAs for numerical features in train : \" + str(df_num.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_num.shape)\n",
    "df_num = df_num.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hstogram\n",
    "sns.distplot(df_num['Number of machinery']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize numerical features\n",
    "### Scale to [0;1] (give equal weight to all features, assuming a linear distribution of values and no significant outliers)\n",
    "### Shift mean to 0 (to center the data set; this will not affect the output at all for most algorithms.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical features\n",
    "features = df_num.columns\n",
    "stdSc = StandardScaler()\n",
    "df_num = stdSc.fit_transform(df_num)\n",
    "df_num = pd.DataFrame.from_records(df_num)\n",
    "df_num.columns = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-effect model preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed_effect_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new data frame with one hot encoded columns for the counties\n",
    "if effect_model == 1:\n",
    "    df_all = pd.concat([df_num, df_cat], axis=1)\n",
    "    figsize = (30, 50)\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=10, ncols=1)\n",
    "    top10 = [\"Number of machinery\", \"Fläche [ha]\", \"Ackerbaubetriebe\", \"Betriebe 20-50 ha\", \"LW-Fläche 20-50 ha\",\n",
    "             \"LW-Fläche 50-100 ha\", \"101-200\", \"Betriebe Rinder\", \"Grünlandfläche [ha]\", \"Betriebe 50-100 ha\"]\n",
    "    for i, top in enumerate(top10):\n",
    "        sns.catplot(x=\"Company\", y=top, data=df_all, kind = \"swarm\", ax=axes[i])\n",
    "    plt.close(2)\n",
    "    Company = df_all.Company.unique()\n",
    "    df_num = pd.concat([df_num, pd.get_dummies(df_all.Company)],axis=1)\n",
    "    df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-effect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge statsmodels -y\n",
    "if effect_model == 2:\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "    # construct our model, with our county now shown as a group\n",
    "    md = smf.mixedlm(\"Q('Number of machinery') ~ \\\n",
    "                     Q('Fläche [ha]') + \\\n",
    "                     Ackerbaubetriebe + \\\n",
    "                     Q('Betriebe 20-50 ha') + \\\n",
    "                     Q('LW-Fläche 50-100 ha') + \\\n",
    "                     Q('LW-Fläche 50-100 ha')\",\n",
    "                     df_all, groups=df_all[\"Company\"])\n",
    "    mdf = md.fit()\n",
    "    print(mdf.summary())\n",
    "    # and let's store the rmse\n",
    "    from math import sqrt\n",
    "    y_predict = mdf.fittedvalues\n",
    "    RMSE = sqrt(((df_all['Number of machinery']-y_predict)**2).values.mean())\n",
    "    results = pd.DataFrame()\n",
    "    results[\"Method\"] = [\"Mixed\"]\n",
    "    results[\"RMSE\"] = RMSE\n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# differentiate dependent and indenpendent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outliers > 0:\n",
    "    df_num = pd.concat([df_num, df_dummy], axis=1)\n",
    "    print(df_num.shape)\n",
    "y = df_num[\"Number of machinery\"]\n",
    "y_features = df_num.columns\n",
    "y_features = y_features.drop(\"Number of machinery\")\n",
    "df_num = df_num[y_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if numFeatures == 1:\n",
    "    nonfive_features = df_num.columns\n",
    "    top5 = [\"Fläche [ha]\", \"Ackerbaubetriebe\", \"Betriebe 20-50 ha\", \"LW-Fläche 20-50 ha\", 'LW-Fläche 50-100 ha']\n",
    "    nonfive_features = nonfive_features.drop(top5)\n",
    "    df_num.drop(labels=nonfive_features, axis=1, inplace=True)\n",
    "elif numFeatures == 2:\n",
    "    nonten_features = df_num.columns\n",
    "    top10 = [\"Company\", \"Fläche [ha]\", \"Ackerbaubetriebe\", \"Betriebe 20-50 ha\", \"LW-Fläche 20-50 ha\",\n",
    "     \"LW-Fläche 50-100 ha\", \"101-200\", \"Betriebe Rinder\", \"Grünlandfläche [ha]\", \"Betriebe 50-100 ha\"]\n",
    "    nonten_features = nonten_features.drop(top10)\n",
    "    df_num.drop(labels=nonten_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join categorical and numerical features\n",
    "# train = pd.concat([df_num, df_cat], axis = 1)\n",
    "train = df_num\n",
    "print(\"New number of features : \" + str(train.shape[1]))\n",
    "\n",
    "# Partition the dataset in train + validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.25, random_state = 0)\n",
    "print(\"X_train : \" + str(X_train.shape))\n",
    "print(\"X_test : \" + str(X_test.shape))\n",
    "print(\"y_train : \" + str(y_train.shape))\n",
    "print(\"y_test : \" + str(y_test.shape))\n",
    "\n",
    "# Define error measure for official scoring : RMSE\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "\n",
    "def rmse_cv_train(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10)) # k-fold, set k as 10\n",
    "    return(rmse)\n",
    "\n",
    "def rmse_cv_test(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1* Linear Regression without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Look at predictions on training and validation set\n",
    "# printing values\n",
    "\n",
    "    \n",
    "if linear_model_type == 0:\n",
    "    linear_model_type_label = 'LinearRegression'\n",
    "    print(linear_model_type_label)\n",
    "elif linear_model_type == 1:\n",
    "    linear_model_type_label = 'RANSACRegression'\n",
    "    print(linear_model_type_label)\n",
    "elif linear_model_type == 2:\n",
    "    linear_model_type_label = 'LinearRegression_Ridge'\n",
    "    print(linear_model_type_label)\n",
    "elif linear_model_type == 3:\n",
    "    linear_model_type_label = 'LinearRegression_Lasso'\n",
    "    print(linear_model_type_label)\n",
    "elif linear_model_type == 4:\n",
    "    linear_model_type_label = 'LinearRegression_ElasticNet'\n",
    "    print(linear_model_type_label)\n",
    "        \n",
    "if spatial_regression == 0:\n",
    "    spatial_regression_label = 'No_SpatialRegression'\n",
    "    print(spatial_regression_label)\n",
    "elif spatial_regression == 1:\n",
    "    spatial_regression_label = 'SpatialRegression'\n",
    "    print(spatial_regression_label)\n",
    "\n",
    "if outliers == 0:\n",
    "    outliers_label = 'without dummy'\n",
    "    print(outliers_label)\n",
    "elif outliers == 1:\n",
    "    outliers_label = 'Dummy for all features'\n",
    "    print(outliers_label)\n",
    "elif outliers == 2:\n",
    "    outliers_label = 'Dummy for each feature'\n",
    "    print(outliers_label)\n",
    "\n",
    "if numFeatures == 0:\n",
    "    numFeatures_label = 'all features'   \n",
    "    print(numFeatures_label)\n",
    "elif numFeatures == 1:\n",
    "    numFeatures_label = 'Top 5 features'\n",
    "    print(numFeatures_label)\n",
    "elif numFeatures == 2:\n",
    "    numFeatures_label = 'Top 10 features'\n",
    "    print(numFeatures_label)\n",
    "    \n",
    "print(transformer_label)\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "if linear_model_type == 0:\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_coef = list(zip(lr.coef_, df_num.columns))\n",
    "    lr_coef.sort(reverse=True)\n",
    "    intercept_ = lr.intercept_\n",
    "    print(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\n",
    "    print(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\n",
    "    y_train_pred = lr.predict(X_train)\n",
    "    y_test_pred = lr.predict(X_test)\n",
    "    df_lr_coef = pd.DataFrame.from_records(lr_coef, columns=['coef', 'features'])\n",
    "elif linear_model_type == 1:\n",
    "    lr = RANSACRegressor()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_coef = list(zip(lr.estimator_.coef_, df_num.columns))\n",
    "    lr_coef.sort(reverse=True)\n",
    "    intercept_ = lr.estimator_.intercept_\n",
    "    print(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\n",
    "    print(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\n",
    "    y_train_pred = lr.predict(X_train)\n",
    "    y_test_pred = lr.predict(X_test)\n",
    "    df_lr_coef = pd.DataFrame.from_records(lr_coef, columns=['coef', 'features'])\n",
    "\n",
    "# 2* Ridge\n",
    "elif linear_model_type == 2:\n",
    "    ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\n",
    "    ridge.fit(X_train, y_train)\n",
    "    alpha = ridge.alpha_\n",
    "    print(\"Best alpha :\", alpha)\n",
    "\n",
    "    print(\"Try again for more precision with alphas centered around \" + str(alpha))\n",
    "    ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n",
    "                              alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n",
    "                              alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n",
    "                    cv = 10)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    alpha = ridge.alpha_\n",
    "    print(\"Best alpha :\", alpha)\n",
    "\n",
    "    print(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\n",
    "    print(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\n",
    "    y_train_rdg = ridge.predict(X_train)\n",
    "    y_test_rdg = ridge.predict(X_test)\n",
    "\n",
    "    y_train_pred = ridge.predict(X_train)\n",
    "    y_test_pred = ridge.predict(X_test)\n",
    "\n",
    "    ridge_coef = list(zip(ridge.coef_, df_num.columns))\n",
    "    ridge_coef.sort(reverse=True)\n",
    "    df_ridge_coef = pd.DataFrame.from_records(ridge_coef, columns=['coef', 'features'])\n",
    "    intercept_ = ridge.intercept_\n",
    "    df_lr_coef =  df_ridge_coef\n",
    "# 3* Lasso\n",
    "elif linear_model_type == 3:\n",
    "\n",
    "    lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n",
    "                              0.3, 0.6, 1], \n",
    "                    max_iter = 50000, cv = 10)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    alpha = lasso.alpha_\n",
    "    print(\"Best alpha :\", alpha)\n",
    "\n",
    "    print(\"Try again for more precision with alphas centered around \" + str(alpha))\n",
    "    lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n",
    "                              alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n",
    "                              alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n",
    "                              alpha * 1.4], \n",
    "                    max_iter = 50000, cv = 10)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    alpha = lasso.alpha_\n",
    "    print(\"Best alpha :\", alpha)\n",
    "\n",
    "    print(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\n",
    "    print(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\n",
    "    y_train_las = lasso.predict(X_train)\n",
    "    y_test_las = lasso.predict(X_test)\n",
    "\n",
    "    y_train_pred = lasso.predict(X_train)\n",
    "    y_test_pred = lasso.predict(X_test)\n",
    "\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    lasso_coef = list(zip(lasso.coef_, df_num.columns))\n",
    "    lasso_coef.sort(reverse=True)\n",
    "    df_lasso_coef = pd.DataFrame.from_records(lasso_coef, columns=['coef', 'features'])\n",
    "    intercept_ = lasso.intercept_\n",
    "    df_lr_coef =  df_lasso_coef\n",
    "# 4* ElasticNet    \n",
    "elif linear_model_type == 4:  \n",
    "    elasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n",
    "                              alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n",
    "                                        0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n",
    "                              max_iter = 50000, cv = 10)\n",
    "    elasticNet.fit(X_train, y_train)\n",
    "    alpha = elasticNet.alpha_\n",
    "    ratio = elasticNet.l1_ratio_\n",
    "    print(\"Best l1_ratio :\", ratio)\n",
    "    print(\"Best alpha :\", alpha )\n",
    "\n",
    "    print(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\n",
    "    elasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n",
    "                              alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n",
    "                              max_iter = 50000, cv = 10)\n",
    "    elasticNet.fit(X_train, y_train)\n",
    "    if (elasticNet.l1_ratio_ > 1):\n",
    "        elasticNet.l1_ratio_ = 1    \n",
    "    alpha = elasticNet.alpha_\n",
    "    ratio = elasticNet.l1_ratio_\n",
    "    print(\"Best l1_ratio :\", ratio)\n",
    "    print(\"Best alpha :\", alpha )\n",
    "\n",
    "    print(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n",
    "          \" and alpha centered around \" + str(alpha))\n",
    "    elasticNet = ElasticNetCV(l1_ratio = ratio,\n",
    "                              alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n",
    "                                        alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n",
    "                                        alpha * 1.35, alpha * 1.4], \n",
    "                              max_iter = 50000, cv = 10)\n",
    "    elasticNet.fit(X_train, y_train)\n",
    "    if (elasticNet.l1_ratio_ > 1):\n",
    "        elasticNet.l1_ratio_ = 1    \n",
    "    alpha = elasticNet.alpha_\n",
    "    ratio = elasticNet.l1_ratio_\n",
    "    print(\"Best l1_ratio :\", ratio)\n",
    "    print(\"Best alpha :\", alpha )\n",
    "\n",
    "    print(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\n",
    "    print(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\n",
    "    y_train_pred = elasticNet.predict(X_train)\n",
    "    y_test_pred = elasticNet.predict(X_test)\n",
    "\n",
    "    elasticNet_coef = list(zip(elasticNet.coef_, df_num.columns))\n",
    "    elasticNet_coef.sort(reverse=True)\n",
    "    df_elasticNet_coef = pd.DataFrame.from_records(elasticNet_coef, columns=['coef', 'features'])\n",
    "    intercept_ = elasticNet.intercept_\n",
    "    df_lr_coef = df_elasticNet_coef\n",
    "#############\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "\n",
    "print('Intercept:', intercept_)\n",
    "print('R2 score of train: ', r2_train)\n",
    "print('R2 score of test: ', r2_test)\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
    "plt.title(\"Linear regression\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.hlines(y = 0, xmin = 0, xmax = 8, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions\n",
    "plt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "plt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
    "plt.title(\"Linear regression\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Real values\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.plot([0, 8], [0, 8], c = \"red\")\n",
    "plt.show()\n",
    "\n",
    "top10_coef_features = []\n",
    "for feature in df_lr_coef.nlargest(k-1, 'coef').features:\n",
    "    top10_coef_features.append(feature)\n",
    "print(\"top10_coef_features\", top10_coef_features)\n",
    "\n",
    "top10_corr_features = []\n",
    "for feature in cols:\n",
    "    if feature == 'Number of machinery':\n",
    "        continue\n",
    "    top10_corr_features.append(feature)\n",
    "print(\"top10_corr_features\", top10_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr_coef.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "sns.set()\n",
    "cols = top10_coef_features\n",
    "sns.pairplot(df_num[cols])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "sns.set()\n",
    "cols = top10_corr_features\n",
    "sns.pairplot(df_num[cols])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts in Spatial Linear Modelling\n",
    "\n",
    "### Data Borrowing in Supervised Learning\n",
    "# Kernel Regressions\n",
    "Kernel regressions are one exceptionally common way to allow observations to \"borrow strength\" from nearby observations. \n",
    "\n",
    "However, when working with spatial data, there are *two simultaneous senses of what is near:* \n",
    "- things that similar in attribute (classical kernel regression)\n",
    "- things that are similar in spatial position (spatial kernel regression)\n",
    "\n",
    "Below, we'll walk through how to use scikit to fit these two types of kernel regressions, show how it's not super simple to mix the two approaches together, and refer to an approach that does this correctly in another package. \n",
    "\n",
    "One method that can exploit the fact that local data may be more informative in predicting $y$ at site $i$ than distant data is Geographically Weighted Regression, a type of Generalized Additive Spatial Model. Kind of like a Kernel Regression, GWR conducts a bunch of regressions at each training site only considering data near that site. This means it works like the kernel regressions above, but uses *both* the coordinates *and* the data in $X$ to predict $y$ at each site. It optimizes its sense of \"local\" depending on some information criteria or fit score.\n",
    "\n",
    "You can find this in the `gwr` package, and significant development is ongoing on this at `https://github.com/pysal/gwr`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
