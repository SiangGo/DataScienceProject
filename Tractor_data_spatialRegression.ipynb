{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler #\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, RANSACRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "from scipy.stats import skew\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as skn\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "import libpysal as lp\n",
    "import geopandas\n",
    "import shapely.geometry as shp\n",
    "\n",
    "from libpysal.weights.util import fill_diagonal\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_type = 2 # LinearRegression = 0, RANSACRegressor = 1, Spatial Regression = 2\n",
    "\n",
    "spatial_numfeatures = 1\n",
    "    \n",
    "outlier_way = 1 # 0: without dummy, 1: dummy variable for all, 2: dummy variables for each feature\n",
    "\n",
    "numFeatures = 0 # 0: considering all features, 1: considering onty top 5 features from correlation matrix, 2: considering onty top 10 features  from correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"df_Merge_ASDnVehiclenCompany_noMissingCol.xlsx\")\n",
    "# df.drop(columns='Unnamed: 0', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "idsUnique = len(set(df.Landkreis))\n",
    "idsTotal = df.shape[0]\n",
    "idsDupli = idsTotal - idsUnique\n",
    "print(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find most important features relative to target\n",
    "print(\"Find most important features relative to target\")\n",
    "corr = df.corr()\n",
    "corr.sort_values([\"Number of machinery\"], ascending = False, inplace = True)\n",
    "corr[\"Number of machinery\"].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle remaining missing values for numerical features by using median as replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NAs for numerical features in train : \" + str(df.isnull().values.sum()))\n",
    "# df_num = df_num.fillna(0)\n",
    "# print(\"Remaining NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if linear_model_type == 2:\n",
    "    landkreise = geopandas.read_file(\"zip://./geodata/landkreise-in-germany.zip\")\n",
    "\n",
    "    landkreise_feature = landkreise.columns\n",
    "    useless_feature = landkreise_feature.drop(['geometry', 'id_2'])\n",
    "    useless_feature\n",
    "\n",
    "    landkreise.drop(useless_feature, axis=1, inplace=True)\n",
    "    landkreise.rename(columns={'id_2':'ID'}, inplace=True)\n",
    "\n",
    "    df_temp = pd.read_excel('preprocessing_landkreisGeometry.xlsx')\n",
    "    landkreisenGeometry = pd.merge(landkreise, df_temp, on=\"ID\", how='inner')\n",
    "    landkreisenGeometry.drop(labels='LKR-ID', axis=1, inplace=True)\n",
    "    landkreisenGeometry\n",
    "\n",
    "    df = pd.merge(landkreisenGeometry, df, on=\"Landkreis\", how='left')\n",
    "    df = df.drop(['ID'], axis=1)\n",
    "    \n",
    "print(df.shape)\n",
    "print(\"NAs for numerical features in train : \" + str(df.isnull().values.sum()))\n",
    "df = df.fillna(0)\n",
    "print(\"Remaining NAs for numerical features in train : \" + str(df.isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiate numerical features (minus the target) and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include = [\"object\"]).columns\n",
    "# categorical_features = categorical_features.drop(['geometry'])\n",
    "numerical_features = df.select_dtypes(exclude = [\"object\"]).columns\n",
    "print(\"Numerical features : \" + str(len(numerical_features)))\n",
    "print(\"Categorical features : \" + str(len(categorical_features)))\n",
    "df_num = df[numerical_features]\n",
    "df_cat = df[categorical_features]\n",
    "df_num.drop(columns=['LKR-ID'], inplace=True) # LKR-ID is just a classification number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if linear_model_type == 2:\n",
    "\n",
    "    \n",
    "    \n",
    "    kW = lp.weights.Kernel.from_dataframe(df_geometry, fixed=False, function='gaussian', k=100)\n",
    "    kW.transform = 'r'\n",
    "    kW = fill_diagonal(kW, 0)\n",
    "\n",
    "    # onlyX = sm.OLS(y,sm.add_constant(Xtable)).fit()\n",
    "    # onlyX.summary()\n",
    "   \n",
    "    if spatial_numfeatures == 0:\n",
    "        spatial_features = df_num.columns    \n",
    "    elif spatial_numfeatures == 1:\n",
    "        spatial_features = ['Number of machinery']\n",
    "    \n",
    "    X = df[spatial_features]\n",
    "    WX = lp.weights.lag_spatial(kW, X)\n",
    "\n",
    "#     Xtable = pd.DataFrame(df_num, columns=df_num.columns)\n",
    "    WXtable = pd.DataFrame(WX, columns=['lag_{}'.format(name) for name in spatial_features])\n",
    "    df_num = pd.concat((df_num,WXtable),axis=1)\n",
    "    \n",
    "print(df_num.shape)\n",
    "# withWX = sm.OLS(y,sm.add_constant(XWXtable)).fit()\n",
    "# withWX.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with outliers\n",
    "#### https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "### Dealing with outliers by means of Z value\n",
    "# threshold = 5\n",
    "# df_outliners = df_all[(np.abs(stats.zscore(df_all)) < threshold).all(axis=1)]\n",
    "# print('By means of Z-score, we get df shape = {}'.format(df_outliners.shape))\n",
    "\n",
    "### Dealing with outliers by means of Interquartile\n",
    "###Interquartile is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers.\n",
    "# df_IQR = df_num[~((df_num < (Q1 - 1.5 * IQR)) | (df_all > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "# print('getting rid of outliers By means of IQR, we get df shape = {}'.format(df_IQR.shape))\n",
    "# sns.boxplot(x=df_num['Number of machinery'])\n",
    "\n",
    "### Dealing with outliers by means of dummy variable\n",
    "Q1 = df_num.quantile(0.25)\n",
    "Q3 = df_num.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df_dummy = df_num.copy()\n",
    "if outlier_way == 1:\n",
    "    df_dummy['dummy variable for outliers'] = 0\n",
    "    df_dummy.loc[df_dummy[((df_dummy < (Q1 - 1.5 * IQR)) | \n",
    "                          (df_dummy > (Q3 + 1.5 * IQR))).any(axis=1)].index, ['dummy variable for outliers']] = 1\n",
    "\n",
    "\n",
    "elif outlier_way == 2:\n",
    "    for column in df_num.columns:\n",
    "        df_dummy['DV for ' + column] = 0\n",
    "        df_dummy.loc[df_dummy[((df_dummy[column] < (Q1[column] - 1.5 * IQR[column])) | \n",
    "                          (df_dummy[column] > (Q3[column] + 1.5 * IQR[column])))].index, ['DV for ' + column]] = 1\n",
    "\n",
    "features = df_num.columns\n",
    "df_dummy.drop(columns=features, inplace=True)\n",
    "###\n",
    "print(df_num.shape)\n",
    "print(df_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hstogram\n",
    "sns.distplot(df_num['Number of machinery']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Log transform of the skewed numerical features to lessen impact of outliers\n",
    "# As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\n",
    "skewness = df_num.apply(lambda x: skew(x))\n",
    "skewness = skewness[abs(skewness) > 0.5]\n",
    "print(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\n",
    "skewed_features = skewness.index\n",
    "df_num[skewed_features] = np.log1p(df_num[skewed_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hstogram\n",
    "sns.distplot(df_num['Number of machinery']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize numerical features\n",
    "### Scale to [0;1] (give equal weight to all features, assuming a linear distribution of values and no significant outliers)\n",
    "### Shift mean to 0 (to center the data set; this will not affect the output at all for most algorithms.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical features\n",
    "features = df_num.columns\n",
    "stdSc = StandardScaler()\n",
    "df_num = stdSc.fit_transform(df_num)\n",
    "df_num = pd.DataFrame.from_records(df_num)\n",
    "df_num.columns = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# differentiate dependent and indenpendent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outlier_way > 0:\n",
    "    df_num = pd.concat([df_num, df_dummy], axis=1)\n",
    "    print(df_num.shape)\n",
    "y = df_num[\"Number of machinery\"]\n",
    "y_features = df_num.columns\n",
    "y_features = y_features.drop(\"Number of machinery\")\n",
    "df_num = df_num[y_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if numFeatures == 1:\n",
    "    nonfive_features = df_num.columns\n",
    "    five_features = ['Betriebe Zuchtsauen', 'Ackerbaubetriebe', 'Betriebe 50-100 ha', 'LW-Fläche 50-100 ha', 'Betriebe Schweine']\n",
    "    nonfive_features = nonfive_features.drop(five_features)\n",
    "    df_num.drop(columns=nonfive_features, inplace=True)\n",
    "if numFeatures == 2:\n",
    "    nonten_features = df_num.columns\n",
    "    ten_features = ['Betriebe Zuchtsauen', 'Ackerbaubetriebe', 'Betriebe 50-100 ha', 'LW-Fläche 50-100 ha', 'Betriebe Schweine',\n",
    "                'Sum of Number of Tractors', 'Viehbestand GV', 'Anzahl Zuchtsauen', 'Lw. Betriebe gesamt', 'Betriebe Viehhaltung']\n",
    "    nonten_features = nonten_features.drop(ten_features)\n",
    "    df_num.drop(columns=nonten_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join categorical and numerical features\n",
    "# train = pd.concat([df_num, df_cat], axis = 1)\n",
    "train = df_num\n",
    "print(\"New number of features : \" + str(train.shape[1]))\n",
    "\n",
    "# Partition the dataset in train + validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.25, random_state = 0)\n",
    "print(\"X_train : \" + str(X_train.shape))\n",
    "print(\"X_test : \" + str(X_test.shape))\n",
    "print(\"y_train : \" + str(y_train.shape))\n",
    "print(\"y_test : \" + str(y_test.shape))\n",
    "\n",
    "# Define error measure for official scoring : RMSE\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "\n",
    "def rmse_cv_train(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10)) # k-fold, set k as 10\n",
    "    return(rmse)\n",
    "\n",
    "def rmse_cv_test(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1* Linear Regression without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at predictions on training and validation set\n",
    "# printing values\n",
    "\n",
    "# Linear Regression\n",
    "if linear_model_type == 0 or linear_model_type == 2:\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_coef = list(zip(lr.coef_, df_num.columns))\n",
    "    lr_coef.sort(reverse=True)\n",
    "    intercept_ = lr.intercept_\n",
    "\n",
    "if linear_model_type == 1:\n",
    "    lr = RANSACRegressor()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_coef = list(zip(lr.estimator_.coef_, df_num.columns))\n",
    "    lr_coef.sort(reverse=True)\n",
    "    intercept_ = lr.estimator_.intercept_\n",
    "    \n",
    "if linear_model_type == 0:\n",
    "    print('LinearRegression')\n",
    "elif linear_model_type == 1:\n",
    "    print('RANSACRegression')\n",
    "elif linear_model_type == 2:\n",
    "    print('SpatialRegression')\n",
    "if outlier_way == 0:\n",
    "    print('without dummy')\n",
    "elif outlier_way == 1:\n",
    "    print('Dummy for all features')\n",
    "elif outlier_way == 2:\n",
    "    print('Dummy for each feature')\n",
    "if numFeatures == 1:\n",
    "    print('Considering only Top 5 features')\n",
    "elif numFeatures == 2:\n",
    "    print('Considering only Top 10 features')\n",
    "\n",
    "    \n",
    "print(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\n",
    "print(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "df_lr_coef = pd.DataFrame.from_records(lr_coef, columns=['coef', 'features'])\n",
    "print('Intercept:', intercept_)\n",
    "print('R2 score of train: ', r2_train)\n",
    "print('R2 score of test: ', r2_test)\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
    "plt.title(\"Linear regression\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.hlines(y = 0, xmin = 0, xmax = 8, color = \"red\")\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions\n",
    "plt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n",
    "plt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n",
    "plt.title(\"Linear regression\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Real values\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.plot([0, 8], [0, 8], c = \"red\")\n",
    "plt.show()\n",
    "\n",
    "df_lr_coef.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts in Spatial Linear Modelling\n",
    "\n",
    "### Data Borrowing in Supervised Learning\n",
    "# Kernel Regressions\n",
    "Kernel regressions are one exceptionally common way to allow observations to \"borrow strength\" from nearby observations. \n",
    "\n",
    "However, when working with spatial data, there are *two simultaneous senses of what is near:* \n",
    "- things that similar in attribute (classical kernel regression)\n",
    "- things that are similar in spatial position (spatial kernel regression)\n",
    "\n",
    "Below, we'll walk through how to use scikit to fit these two types of kernel regressions, show how it's not super simple to mix the two approaches together, and refer to an approach that does this correctly in another package. \n",
    "\n",
    "One method that can exploit the fact that local data may be more informative in predicting $y$ at site $i$ than distant data is Geographically Weighted Regression, a type of Generalized Additive Spatial Model. Kind of like a Kernel Regression, GWR conducts a bunch of regressions at each training site only considering data near that site. This means it works like the kernel regressions above, but uses *both* the coordinates *and* the data in $X$ to predict $y$ at each site. It optimizes its sense of \"local\" depending on some information criteria or fit score.\n",
    "\n",
    "You can find this in the `gwr` package, and significant development is ongoing on this at `https://github.com/pysal/gwr`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
